{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de0187f-d419-4ac7-9d0d-f1744e4dbb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Good Morning\n"
     ]
    }
   ],
   "source": [
    "print (\"Hello, Good Morning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c25d29-72c4-4f12-9771-7853e62caae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 60ms/step - loss: 0.6925 - accuracy: 0.5875 - val_loss: 0.6921 - val_accuracy: 0.6500\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6919 - accuracy: 0.6250 - val_loss: 0.6915 - val_accuracy: 0.6500\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6914 - accuracy: 0.6625 - val_loss: 0.6909 - val_accuracy: 0.6500\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6908 - accuracy: 0.6625 - val_loss: 0.6903 - val_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6903 - accuracy: 0.7375 - val_loss: 0.6897 - val_accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6897 - accuracy: 0.7500 - val_loss: 0.6891 - val_accuracy: 0.7500\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6892 - accuracy: 0.7625 - val_loss: 0.6885 - val_accuracy: 0.7500\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6887 - accuracy: 0.7625 - val_loss: 0.6879 - val_accuracy: 0.7500\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6881 - accuracy: 0.7625 - val_loss: 0.6874 - val_accuracy: 0.7500\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6876 - accuracy: 0.7625 - val_loss: 0.6868 - val_accuracy: 0.7500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6868 - accuracy: 0.7500\n",
      "Test Loss: 0.6868, Test Accuracy: 0.7500\n",
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Generate random data for illustration\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = (X > 0.5).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a simple neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797c06b5-3e47-415c-8a37-79fbf1a55879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 3s 0us/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 08:08:41.119360: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 150528000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749/750 [============================>.] - ETA: 0s - loss: 0.2137 - accuracy: 0.9328"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 08:09:05.716103: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 37632000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 27s 34ms/step - loss: 0.2136 - accuracy: 0.9328 - val_loss: 0.0718 - val_accuracy: 0.9787\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 25s 33ms/step - loss: 0.0589 - accuracy: 0.9814 - val_loss: 0.0488 - val_accuracy: 0.9866\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 26s 35ms/step - loss: 0.0411 - accuracy: 0.9870 - val_loss: 0.0424 - val_accuracy: 0.9879\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 23s 30ms/step - loss: 0.0293 - accuracy: 0.9904 - val_loss: 0.0457 - val_accuracy: 0.9877\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 22s 29ms/step - loss: 0.0242 - accuracy: 0.9923 - val_loss: 0.0373 - val_accuracy: 0.9900\n",
      " 24/313 [=>............................] - ETA: 1s - loss: 0.0274 - accuracy: 0.9896"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 08:10:43.842418: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 31360000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0300 - accuracy: 0.9898\n",
      "Test Accuracy: 0.9898\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Build the neural network model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13811bd2-971b-442f-96d8-1277211e34f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.datasets' has no attribute 'mnlist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mmnist\u001b[38;5;241m.\u001b[39mload_data(\n\u001b[1;32m      3\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist.npz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[0;32m----> 7\u001b[0m (train_images, train_labels), (test_images, test_labels) \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmnlist\u001b[49m\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m      9\u001b[0m train_images \u001b[38;5;241m=\u001b[39m train_images\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m60000\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m     10\u001b[0m test_images \u001b[38;5;241m=\u001b[39m test_images\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.datasets' has no attribute 'mnlist'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.datasets.mnist.load_data(\n",
    "    path='mnist.npz'\n",
    ")\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnlist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "print(train_labels)\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54659f69-577c-4c5a-b86a-5c052d34c957",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.utils.data_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MNIST handwritten digits dataset.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_file\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.utils.data_utils'"
     ]
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"MNIST handwritten digits dataset.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "# isort: off\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "@keras_export(\"keras.datasets.mnist.load_data\")\n",
    "def load_data(path=\"mnist.npz\"):\n",
    "    \"\"\"Loads the MNIST dataset.\n",
    "\n",
    "    This is a dataset of 60,000 28x28 grayscale images of the 10 digits,\n",
    "    along with a test set of 10,000 images.\n",
    "    More info can be found at the\n",
    "    [MNIST homepage](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "    Args:\n",
    "      path: path where to cache the dataset locally\n",
    "        (relative to `~/.keras/datasets`).\n",
    "\n",
    "    Returns:\n",
    "      Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "\n",
    "    **x_train**: uint8 NumPy array of grayscale image data with shapes\n",
    "      `(60000, 28, 28)`, containing the training data. Pixel values range\n",
    "      from 0 to 255.\n",
    "\n",
    "    **y_train**: uint8 NumPy array of digit labels (integers in range 0-9)\n",
    "      with shape `(60000,)` for the training data.\n",
    "\n",
    "    **x_test**: uint8 NumPy array of grayscale image data with shapes\n",
    "      (10000, 28, 28), containing the test data. Pixel values range\n",
    "      from 0 to 255.\n",
    "\n",
    "    **y_test**: uint8 NumPy array of digit labels (integers in range 0-9)\n",
    "      with shape `(10000,)` for the test data.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```python\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    assert x_train.shape == (60000, 28, 28)\n",
    "    assert x_test.shape == (10000, 28, 28)\n",
    "    assert y_train.shape == (60000,)\n",
    "    assert y_test.shape == (10000,)\n",
    "    ```\n",
    "\n",
    "    License:\n",
    "      Yann LeCun and Corinna Cortes hold the copyright of MNIST dataset,\n",
    "      which is a derivative work from original NIST datasets.\n",
    "      MNIST dataset is made available under the terms of the\n",
    "      [Creative Commons Attribution-Share Alike 3.0 license.](\n",
    "      https://creativecommons.org/licenses/by-sa/3.0/)\n",
    "    \"\"\"\n",
    "    origin_folder = (\n",
    "        \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n",
    "    )\n",
    "    path = get_file(\n",
    "        path,\n",
    "        origin=origin_folder + \"mnist.npz\",\n",
    "        file_hash=(  # noqa: E501\n",
    "            \"731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1\"\n",
    "        ),\n",
    "    )\n",
    "    with np.load(path, allow_pickle=True) as f:\n",
    "        x_train, y_train = f[\"x_train\"], f[\"y_train\"]\n",
    "        x_test, y_test = f[\"x_test\"], f[\"y_test\"]\n",
    "\n",
    "        return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11119620-a0d2-4b57-a8f5-10595b33a0b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_utils\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# build your model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m(input_shape, num_classes\u001b[38;5;241m=\u001b[39mnum_classes, reserve_layers\u001b[38;5;241m=\u001b[39mreserve_layers)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# build your train-set & validation-set generators\u001b[39;00m\n\u001b[1;32m      8\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m ImageDataGenerator()\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m      9\u001b[0m         train_images_path,\n\u001b[1;32m     10\u001b[0m         target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_model' is not defined"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras_utils\n",
    "\n",
    "# build your model\n",
    "model = get_model(input_shape, num_classes=num_classes, reserve_layers=reserve_layers)\n",
    "\n",
    "# build your train-set & validation-set generators\n",
    "train_generator = ImageDataGenerator().flow_from_directory(\n",
    "        train_images_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        seed=seed,\n",
    "        shuffle=True\n",
    ")\n",
    "\n",
    "valid_generator = ImageDataGenerator().flow_from_directory(\n",
    "        valid_images_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=1,\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    ")\n",
    "\n",
    "# create a validation-callback which tests the validation-set every 10 epocks\n",
    "valid_callback = ValidateCallback(10, valid_generator, model)\n",
    "\n",
    "# create a save-callback which saves the model every 20 epochs under the name \"model_name\"\n",
    "save_callback = SaveCallback(20, 'model_name.model', model)\n",
    "\n",
    "# train your model\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    workers=8,\n",
    "                    callbacks=[valid_callback, save_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
